{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import lmdb\n",
    "from path import Path\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf1\n",
    "import json\n",
    "import editdistance\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple\n",
    "import tensorflow as tf\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Create lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir= \"/data/linemode\"\n",
    "data_dir11= \"/data/\"\n",
    "f = open(data_dir11 + '/words0.txt')\n",
    "chars = set()\n",
    "fn_imgs=[]\n",
    "\n",
    "if not os.path.exists(data_dir + 'lmdb'):\n",
    "      env = lmdb.open(str(data_dir + 'lmdb'), map_size=1024 * 1024 * 1024 * 2)\n",
    "for line in f:\n",
    " # ignore comment line\n",
    "    if not line or line[0] == '#':\n",
    "        continue\n",
    "\n",
    "    line_split = line.strip().split(' ')\n",
    "    assert len(line_split) >= 9\n",
    "\n",
    "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "    file_name_split = line_split[0].split('-')\n",
    "    file_name_subdir1 = file_name_split[0]\n",
    "    file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
    "    file_base_name = line_split[0] + '.png'\n",
    "    file_name = data_dir11 + 'unzipedwords/words/' + file_name_subdir1+ '/' + file_name_subdir2 + '/' + file_base_name\n",
    "    fn_imgs.append(file_name)\n",
    "with env.begin(write=True) as txn:\n",
    "      for i, fn_img in enumerate(fn_imgs):\n",
    "            img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
    "            basename = os.path.basename(fn_img)\n",
    "            txn.put(basename.encode(\"ascii\"), pickle.dumps(img))\n",
    "\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
    "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')\n",
    "data_dir1= \"/data/linemode\"\n",
    "\n",
    "class DataLoaderIAM:\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dir: Path,\n",
    "                 batch_size: int,\n",
    "                 data_split: float = 0.95,\n",
    "                 fast: bool = True\n",
    "                 ) -> None:\n",
    "        #Loader for dataset\n",
    "\n",
    "        assert os.path.exists(data_dir)\n",
    "\n",
    "        self.fast = fast\n",
    "        if fast:\n",
    "            self.env = lmdb.open(data_dir1 + 'lmdb', readonly=True)\n",
    "\n",
    "        self.data_augmentation = False\n",
    "        self.curr_idx = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.samples = []\n",
    "\n",
    "        f = open(data_dir + '/words0.txt')\n",
    "        chars = set()\n",
    "        bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']  # known broken images in IAM dataset\n",
    "        for line in f:\n",
    "            # ignore comment line\n",
    "            if not line or line[0] == '#':\n",
    "                continue\n",
    "\n",
    "            line_split = line.strip().split(' ')\n",
    "            assert len(line_split) >= 9\n",
    "\n",
    "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "            file_name_split = line_split[0].split('-')\n",
    "            file_name_subdir1 = file_name_split[0]\n",
    "            file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
    "            file_base_name = line_split[0] + '.png'\n",
    "            file_name = data_dir + 'unzipedwords/words/' + file_name_subdir1+ '/' + file_name_subdir2 + '/' + file_base_name\n",
    "\n",
    "            if line_split[0] in bad_samples_reference:\n",
    "                print('Ignoring known broken image:', file_name)\n",
    "                continue\n",
    "\n",
    "            # GT text are columns starting at 9\n",
    "            gt_text = ' '.join(line_split[8:])\n",
    "            chars = chars.union(set(list(gt_text)))\n",
    "\n",
    "            # put sample into list\n",
    "            self.samples.append(Sample(gt_text, file_name))\n",
    "\n",
    "        # split into training and validation set: 95% - 5%\n",
    "        split_idx = int(data_split * len(self.samples))\n",
    "        self.train_samples = self.samples[:split_idx]\n",
    "        self.validation_samples = self.samples[split_idx:]\n",
    "\n",
    "        # put words into lists\n",
    "        self.train_words = [x.gt_text for x in self.train_samples]\n",
    "        self.validation_words = [x.gt_text for x in self.validation_samples]\n",
    "\n",
    "        # start with train set\n",
    "        self.train_set()\n",
    "\n",
    "        # list of all chars in dataset\n",
    "        self.char_list = sorted(list(chars))\n",
    "\n",
    "    def train_set(self) -> None:\n",
    "        #Switch to randomly chosen subset of training set\n",
    "        self.data_augmentation = True\n",
    "        self.curr_idx = 0\n",
    "        random.shuffle(self.train_samples)\n",
    "        self.samples = self.train_samples\n",
    "        self.curr_set = 'train'\n",
    "\n",
    "    def validation_set(self) -> None:\n",
    "       #Switch to validation set\n",
    "        self.data_augmentation = False\n",
    "        self.curr_idx = 0\n",
    "        self.samples = self.validation_samples\n",
    "        self.curr_set = 'val'\n",
    "\n",
    "    def get_iterator_info(self) -> Tuple[int, int]:\n",
    "        #Current batch index and overall number of batches\n",
    "        if self.curr_set == 'train':\n",
    "            num_batches = int(np.floor(len(self.samples) / self.batch_size))  # train set: only full-sized batches\n",
    "        else:\n",
    "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))  # val set: allow last batch to be smaller\n",
    "        curr_batch = self.curr_idx // self.batch_size + 1\n",
    "        return curr_batch, num_batches\n",
    "\n",
    "    def has_next(self) -> bool:\n",
    "        #Is there a next element?\n",
    "        if self.curr_set == 'train':\n",
    "            return self.curr_idx + self.batch_size <= len(self.samples)  # train set: only full-sized batches\n",
    "        else:\n",
    "            return self.curr_idx < len(self.samples)  # val set: allow last batch to be smaller\n",
    "\n",
    "    def _get_img(self, i: int) -> np.ndarray:\n",
    "        if self.fast:\n",
    "            with self.env.begin() as txn:\n",
    "                basename = os.path.basename(self.samples[i].file_path)\n",
    "                data = txn.get(basename.encode(\"ascii\"))\n",
    "                img = pickle.loads(data)\n",
    "        else:\n",
    "            img = cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def get_next(self) -> Batch:\n",
    "        #Get next element\n",
    "        batch_range = range(self.curr_idx, min(self.curr_idx + self.batch_size, len(self.samples)))\n",
    "\n",
    "        imgs = [self._get_img(i) for i in batch_range]\n",
    "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
    "\n",
    "        self.curr_idx += self.batch_size\n",
    "        return Batch(imgs, gt_texts, len(imgs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,\n",
    "                 img_size: Tuple[int, int],\n",
    "                 padding: int = 0,\n",
    "                 dynamic_width: bool = False,\n",
    "                 data_augmentation: bool = False,\n",
    "                 line_mode: bool = True\n",
    "                 ) -> None:\n",
    "        # dynamic width only supported when no data augmentation happens\n",
    "        assert not (dynamic_width and data_augmentation)\n",
    "        # when padding is on, we need dynamic width enabled\n",
    "        assert not (padding > 0 and not dynamic_width)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.padding = padding\n",
    "        self.dynamic_width = dynamic_width\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.line_mode = line_mode\n",
    "\n",
    "    @staticmethod\n",
    "    def _truncate_label(text: str, max_text_len: int) -> str:\n",
    "        \"\"\"\n",
    "        Function ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
    "        labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "        If a too-long label is provided, ctc_loss returns an infinite gradient.\n",
    "        \"\"\"\n",
    "        cost = 0\n",
    "        for i in range(len(text)):\n",
    "            if i != 0 and text[i] == text[i - 1]:\n",
    "                cost += 2\n",
    "            else:\n",
    "                cost += 1\n",
    "            if cost > max_text_len:\n",
    "                return text[:i]\n",
    "        return text\n",
    "\n",
    "    def _simulate_text_line(self, batch: Batch) -> Batch:\n",
    "       #Create image of a text line by pasting multiple word images into an image\n",
    "\n",
    "        default_word_sep = 30\n",
    "        default_num_words = 5\n",
    "\n",
    "        # go over all batch elements\n",
    "        res_imgs = []\n",
    "        res_gt_texts = []\n",
    "        for i in range(batch.batch_size):\n",
    "            # number of words to put into current line\n",
    "            num_words = random.randint(1, 8) if self.data_augmentation else default_num_words\n",
    "\n",
    "            # concat ground truth texts\n",
    "            curr_gt = ' '.join([batch.gt_texts[(i + j) % batch.batch_size] for j in range(num_words)])\n",
    "            res_gt_texts.append(curr_gt)\n",
    "\n",
    "            # put selected word images into list, compute target image size\n",
    "            sel_imgs = []\n",
    "            word_seps = [0]\n",
    "            h = 0\n",
    "            w = 0\n",
    "            for j in range(num_words):\n",
    "                curr_sel_img = batch.imgs[(i + j) % batch.batch_size]\n",
    "                curr_word_sep = random.randint(20, 50) if self.data_augmentation else default_word_sep\n",
    "                h = max(h, curr_sel_img.shape[0])\n",
    "                w += curr_sel_img.shape[1]\n",
    "                sel_imgs.append(curr_sel_img)\n",
    "                if j + 1 < num_words:\n",
    "                    w += curr_word_sep\n",
    "                    word_seps.append(curr_word_sep)\n",
    "\n",
    "            # put all selected word images into target image\n",
    "            target = np.ones([h, w], np.uint8) * 255\n",
    "            x = 0\n",
    "            for curr_sel_img, curr_word_sep in zip(sel_imgs, word_seps):\n",
    "                x += curr_word_sep\n",
    "                y = (h - curr_sel_img.shape[0]) // 2\n",
    "                target[y:y + curr_sel_img.shape[0]:, x:x + curr_sel_img.shape[1]] = curr_sel_img\n",
    "                x += curr_sel_img.shape[1]\n",
    "\n",
    "            # put image of line into result\n",
    "            res_imgs.append(target)\n",
    "        \n",
    "        \n",
    "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
    "\n",
    "    def process_img(self, img: np.ndarray) -> np.ndarray:\n",
    "        #Resize to target size, apply data augmentation\n",
    "\n",
    "        # there are damaged files in IAM dataset - just use black image instead\n",
    "        if img is None:\n",
    "            img = np.zeros(self.img_size[::-1])\n",
    "\n",
    "        # data augmentation\n",
    "        img = img.astype(np.float)\n",
    "        if self.data_augmentation:\n",
    "            # photometric data augmentation\n",
    "            if random.random() < 0.25:\n",
    "                def rand_odd():\n",
    "                    return random.randint(1, 3) * 2 + 1\n",
    "                img = cv2.GaussianBlur(img, (rand_odd(), rand_odd()), 0)\n",
    "            if random.random() < 0.25:\n",
    "                img = cv2.dilate(img, np.ones((3, 3)))\n",
    "            if random.random() < 0.25:\n",
    "                img = cv2.erode(img, np.ones((3, 3)))\n",
    "\n",
    "            # geometric data augmentation\n",
    "            wt, ht = self.img_size\n",
    "            h, w = img.shape\n",
    "            f = min(wt / w, ht / h)\n",
    "            fx = f * np.random.uniform(0.75, 1.05)\n",
    "            fy = f * np.random.uniform(0.75, 1.05)\n",
    "\n",
    "            # random position around center\n",
    "            txc = (wt - w * fx) / 2\n",
    "            tyc = (ht - h * fy) / 2\n",
    "            freedom_x = max((wt - fx * w) / 2, 0)\n",
    "            freedom_y = max((ht - fy * h) / 2, 0)\n",
    "            tx = txc + np.random.uniform(-freedom_x, freedom_x)\n",
    "            ty = tyc + np.random.uniform(-freedom_y, freedom_y)\n",
    "\n",
    "            # map image into target image\n",
    "            M = np.float32([[fx, 0, tx], [0, fy, ty]])\n",
    "            target = np.ones(self.img_size[::-1]) * 255\n",
    "            img = cv2.warpAffine(img, M, dsize=self.img_size, dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "\n",
    "            # photometric data augmentation\n",
    "            if random.random() < 0.5:\n",
    "                img = img * (0.25 + random.random() * 0.75)\n",
    "            if random.random() < 0.25:\n",
    "                img = np.clip(img + (np.random.random(img.shape) - 0.5) * random.randint(1, 25), 0, 255)\n",
    "            if random.random() < 0.1:\n",
    "                img = 255 - img\n",
    "\n",
    "        # no data augmentation\n",
    "        else:\n",
    "            if self.dynamic_width:\n",
    "                ht = self.img_size[1]\n",
    "                h, w = img.shape\n",
    "                f = ht / h\n",
    "                wt = int(f * w + self.padding)\n",
    "                wt = wt + (4 - wt) % 4\n",
    "                tx = (wt - w * f) / 2\n",
    "                ty = 0\n",
    "            else:\n",
    "                wt, ht = self.img_size\n",
    "                h, w = img.shape\n",
    "                f = min(wt / w, ht / h)\n",
    "                tx = (wt - w * f) / 2\n",
    "                ty = (ht - h * f) / 2\n",
    "\n",
    "            # map image into target image\n",
    "            M = np.float32([[f, 0, tx], [0, f, ty]])\n",
    "            target = np.ones([ht, wt]) * 255\n",
    "            img = cv2.warpAffine(img, M, dsize=(wt, ht), dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
    "\n",
    "        # transpose for TF\n",
    "        img = cv2.transpose(img)\n",
    "\n",
    "        # convert to range [-1, 1]\n",
    "        img = img / 255 - 0.5\n",
    "        return img\n",
    "\n",
    "    def process_batch(self, batch: Batch) -> Batch:\n",
    "\n",
    "        if self.line_mode:\n",
    "            batch = self._simulate_text_line(batch)\n",
    "            \n",
    "\n",
    "        res_imgs = [self.process_img(img) for img in batch.imgs]\n",
    "        max_text_len = res_imgs[0].shape[0] // 4\n",
    "        res_gt_texts = [self._truncate_label(gt_text, max_text_len) for gt_text in batch.gt_texts]\n",
    "        return Batch(res_imgs, res_gt_texts, batch.batch_size)\n",
    "\n",
    "\n",
    "def main():\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    img = cv2.imread('/data/line.png', cv2.IMREAD_GRAYSCALE)\n",
    "    img_aug = Preprocessor((256, 32), data_augmentation=True).process_img(img)\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(cv2.transpose(img_aug) + 0.5, cmap='gray', vmin=0, vmax=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing image functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_before_cutting(img):\n",
    "    smallest = np.amin(img)\n",
    "    biggest = np.amax(img)\n",
    "    t = 225\n",
    "    (_,img0)=cv2.threshold(img,t,255,cv2.THRESH_BINARY)\n",
    "    kernel=np.ones((3,3), np.uint8)\n",
    "    img0=cv2.morphologyEx(img0, cv2.MORPH_CLOSE, kernel,iterations=1)\n",
    "    return img0\n",
    "\n",
    "def find_white_lines(img):\n",
    "    height , width = img.shape\n",
    "    whites = []\n",
    "    for i in range(height):\n",
    "        if(sum(img[:][i])==255*width):\n",
    "            whites.append(1)\n",
    "        else:\n",
    "            whites.append(0)\n",
    "    return whites\n",
    "\n",
    "def crop_img_into_lines(img):\n",
    "    img = edit_before_cutting(img)\n",
    "    cv2.imwrite('cache.png',img)\n",
    "    img_PIL = Image.open(r\"cache.png\")\n",
    "    img_cv = cv2.imread(r\"cache.png\",0)\n",
    "    height , width = img_cv.shape\n",
    "    imgs = []\n",
    "    left = 0\n",
    "    right = width\n",
    "    whites = find_white_lines(img)\n",
    "    step = 0\n",
    "    top = 0\n",
    "    for i in range(height):\n",
    "        if(step==0):\n",
    "            if(whites[i]==1):\n",
    "                top = i\n",
    "            else:\n",
    "                bottom = i\n",
    "                step = 1\n",
    "        else:\n",
    "            if(whites[i]==0):\n",
    "                bottom = i\n",
    "            else:\n",
    "                bottom = min(i,height-1)\n",
    "                \n",
    "                img0 = img_PIL.crop((left, top, right, bottom))\n",
    "                img0.save(r\"cache.png\")\n",
    "                img00 = cv2.imread('cache.png',0)\n",
    "                imgs.append(img00)\n",
    "                \n",
    "                top = i\n",
    "                step = 0\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def find_white_words(img):\n",
    "    \n",
    "    height , width = img.shape\n",
    "    whites = []\n",
    "    for i in range(0,width,20):\n",
    "        sum1=0\n",
    "        for k in range(20):\n",
    "            for j in range(height):\n",
    "                sum1+=img[j][(i+k)%width]\n",
    "        if(sum1==255*height*20):\n",
    "            for o in range(20):\n",
    "                whites.append(1)\n",
    "        else:\n",
    "            for o in range(20):\n",
    "                whites.append(0)\n",
    "    return whites\n",
    "\n",
    "\n",
    "def crop_lines_into_words(img):\n",
    "    \n",
    "    height , width = img.shape\n",
    "    cv2.imwrite('cache.png',img)\n",
    "    img_PIL = Image.open(r\"cache.png\")\n",
    "    img_cv = cv2.imread(r\"cache.png\",0)\n",
    "    imgs = []\n",
    "    top = 0\n",
    "    bottom = height\n",
    "    whites = find_white_words(img)\n",
    "    step = 0\n",
    "    left = 0\n",
    "    for i in range(width):\n",
    "        if(step==0):\n",
    "            if(whites[i]==1):\n",
    "                left = i\n",
    "            else:\n",
    "                right = i\n",
    "                step = 1\n",
    "        else:\n",
    "            if(whites[i]==0):\n",
    "                right = i\n",
    "            else:\n",
    "                right = min(i,width-1)\n",
    "                \n",
    "                img0 = img_PIL.crop((left, top, right, bottom))\n",
    "                img0.save(r\"cache.png\")\n",
    "                img00 = cv2.imread('cache.png',0)\n",
    "                imgs.append(img00)\n",
    "                \n",
    "                left = i\n",
    "                step = 0\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_correction(text):\n",
    "    return str(TextBlob(text.lower()).correct())\n",
    "\n",
    "def StrToTxtFile(list_text,filename):\n",
    "    string_text = \"\"\n",
    "    for i in list_text :\n",
    "        string_text = string_text + text_correction(i) + \" \"\n",
    "    \n",
    "    filename = filename+\".txt\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(string_text)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_output(output_word_list,recognized):\n",
    "    output_word_list.append(recognized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable eager mode\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "class DecoderType:\n",
    "    #CTC decoder types\n",
    "    BestPath = 0\n",
    "    BeamSearch = 1\n",
    "    WordBeamSearch = 2\n",
    "\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self,\n",
    "                 char_list: List[str],\n",
    "                 decoder_type: str = DecoderType.BestPath,\n",
    "                 must_restore: bool = False,\n",
    "                 dump: bool = False) -> None:\n",
    "        #Init model: add CNN, RNN and CTC and initialize TF\n",
    "        self.dump = dump\n",
    "        self.char_list = char_list\n",
    "        self.decoder_type = decoder_type\n",
    "        self.must_restore = must_restore\n",
    "        self.snap_ID = 0\n",
    "\n",
    "        # Whether to use normalization over a batch or a population\n",
    "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "        # input image batch\n",
    "        self.input_imgs = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))\n",
    "\n",
    "        # setup CNN, RNN and CTC\n",
    "        self.setup_cnn()\n",
    "        self.setup_rnn()\n",
    "        self.setup_ctc()\n",
    "\n",
    "        # setup optimizer to train NN\n",
    "        self.batches_trained = 0\n",
    "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_ops):\n",
    "            self.optimizer = tf.compat.v1.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "        # initialize TF\n",
    "        self.sess, self.saver = self.setup_tf()\n",
    "\n",
    "    def setup_cnn(self) -> None:\n",
    "        #Create CNN layers\n",
    "        cnn_in4d = tf.expand_dims(input=self.input_imgs, axis=3)\n",
    "\n",
    "        # list of parameters for the layers\n",
    "        kernel_vals = [5, 5, 3, 3, 3]\n",
    "        feature_vals = [1, 32, 64, 128, 128, 256]\n",
    "        stride_vals = pool_vals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
    "        num_layers = len(stride_vals)\n",
    "\n",
    "        # create layers\n",
    "        pool = cnn_in4d  # input to first CNN layer\n",
    "        for i in range(num_layers):\n",
    "            kernel = tf.Variable(\n",
    "                tf.random.truncated_normal([kernel_vals[i], kernel_vals[i], feature_vals[i], feature_vals[i + 1]],\n",
    "                                           stddev=0.1))\n",
    "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
    "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
    "            relu = tf.nn.relu(conv_norm)\n",
    "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, pool_vals[i][0], pool_vals[i][1], 1),\n",
    "                                    strides=(1, stride_vals[i][0], stride_vals[i][1], 1), padding='VALID')\n",
    "\n",
    "        self.cnn_out_4d = pool\n",
    "\n",
    "    def setup_rnn(self) -> None:\n",
    "        #Create RNN layers\n",
    "        rnn_in3d = tf.squeeze(self.cnn_out_4d, axis=[2])\n",
    "\n",
    "        # basic cells which is used to build RNN\n",
    "        num_hidden = 256\n",
    "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n",
    "                 range(2)]  # 2 layers\n",
    "\n",
    "        # stack basic cells\n",
    "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # bidirectional RNN\n",
    "        # BxTxF -> BxTx2H\n",
    "        (fw, bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_in3d,\n",
    "                                                                dtype=rnn_in3d.dtype)\n",
    "\n",
    "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "\n",
    "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
    "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, num_hidden * 2, len(self.char_list) + 1], stddev=0.1))\n",
    "        self.rnn_out_3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'),\n",
    "                                     axis=[2])\n",
    "\n",
    "    def setup_ctc(self) -> None:\n",
    "        #Create CTC loss and decoder\n",
    "        # BxTxC -> TxBxC\n",
    "        self.ctc_in_3d_tbc = tf.transpose(a=self.rnn_out_3d, perm=[1, 0, 2])\n",
    "        # ground truth text as sparse tensor\n",
    "        self.gt_texts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
    "                                        tf.compat.v1.placeholder(tf.int32, [None]),\n",
    "                                        tf.compat.v1.placeholder(tf.int64, [2]))\n",
    "\n",
    "        # calc loss for batch\n",
    "        self.seq_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "        self.loss = tf.reduce_mean(\n",
    "            input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.ctc_in_3d_tbc,\n",
    "                                                  sequence_length=self.seq_len,\n",
    "                                                  ctc_merge_repeated=True))\n",
    "\n",
    "        # calc loss for each element to compute label probability\n",
    "        self.saved_ctc_input = tf.compat.v1.placeholder(tf.float32,\n",
    "                                                        shape=[None, None, len(self.char_list) + 1])\n",
    "        self.loss_per_element = tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.saved_ctc_input,\n",
    "                                                         sequence_length=self.seq_len, ctc_merge_repeated=True)\n",
    "\n",
    "        # best path decoding or beam search decoding\n",
    "        if self.decoder_type == DecoderType.BestPath:\n",
    "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len)\n",
    "        elif self.decoder_type == DecoderType.BeamSearch:\n",
    "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len,\n",
    "                                                         beam_width=50)\n",
    "\n",
    "\n",
    "    def setup_tf(self) -> Tuple[tf.compat.v1.Session, tf.compat.v1.train.Saver]:\n",
    "        #Initialize TF\n",
    "        print('Python: ' + sys.version)\n",
    "        print('Tensorflow: ' + tf.__version__)\n",
    "\n",
    "        sess = tf.compat.v1.Session()  # TF session\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
    "        model_dir = '/model/'\n",
    "        latest_snapshot = tf.train.latest_checkpoint(model_dir)  # is there a saved model?\n",
    "\n",
    "        # if model must be restored (for inference), there must be a snapshot\n",
    "        if self.must_restore and not latest_snapshot:\n",
    "            raise Exception('No saved model found in: ' + model_dir)\n",
    "\n",
    "        # load saved model if available\n",
    "        if latest_snapshot:\n",
    "            print('Init with stored values from ' + latest_snapshot)\n",
    "            saver.restore(sess, latest_snapshot)\n",
    "        else:\n",
    "            print('Init with new values')\n",
    "            sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        return sess, saver\n",
    "\n",
    "    def to_sparse(self, texts: List[str]) -> Tuple[List[List[int]], List[int], List[int]]:\n",
    "        #Put ground truth texts into sparse tensor for ctc_loss\n",
    "        indices = []\n",
    "        values = []\n",
    "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
    "\n",
    "        # go over all texts\n",
    "        for batchElement, text in enumerate(texts):\n",
    "            # convert to string of label (i.e. class-ids)\n",
    "            label_str = [self.char_list.index(c) for c in text]\n",
    "            # sparse tensor must have size of max. label-string\n",
    "            if len(label_str) > shape[1]:\n",
    "                shape[1] = len(label_str)\n",
    "            # put each label into sparse tensor\n",
    "            for i, label in enumerate(label_str):\n",
    "                indices.append([batchElement, i])\n",
    "                values.append(label)\n",
    "\n",
    "        return indices, values, shape\n",
    "\n",
    "    def decoder_output_to_text(self, ctc_output: tuple, batch_size: int) -> List[str]:\n",
    "      #Extract texts from output of CTC decoder\n",
    "\n",
    "      # TF decoders: label strings are contained in sparse tensor\n",
    "\n",
    "      # ctc returns tuple, first element is SparseTensor\n",
    "      decoded = ctc_output[0][0]\n",
    "\n",
    "      # contains string of labels for each batch element\n",
    "      label_strs = [[] for _ in range(batch_size)]\n",
    "\n",
    "      # go over all indices and save mapping: batch -> values\n",
    "      for (idx, idx2d) in enumerate(decoded.indices):\n",
    "        label = decoded.values[idx]\n",
    "        batch_element = idx2d[0]  # index according to [b,t]\n",
    "        label_strs[batch_element].append(label)\n",
    "\n",
    "      # map labels to chars for all batch elements\n",
    "      return [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
    "\n",
    "    def train_batch(self, batch: Batch) -> float:\n",
    "        #Feed a batch into the NN to train it\n",
    "        num_batch_elements = len(batch.imgs)\n",
    "        max_text_len = batch.imgs[0].shape[0] // 4\n",
    "        sparse = self.to_sparse(batch.gt_texts)\n",
    "        eval_list = [self.optimizer, self.loss]\n",
    "        feed_dict = {self.input_imgs: batch.imgs, self.gt_texts: sparse,\n",
    "                     self.seq_len: [max_text_len] * num_batch_elements, self.is_train: True}\n",
    "        _, loss_val = self.sess.run(eval_list, feed_dict)\n",
    "        self.batches_trained += 1\n",
    "        return loss_val\n",
    "\n",
    "    @staticmethod\n",
    "    def dump_nn_output(rnn_output: np.ndarray) -> None:\n",
    "        #Dump the output of the NN to CSV file(s)\n",
    "        dump_dir = '/dump/'\n",
    "        if not os.path.isdir(dump_dir):\n",
    "            os.mkdir(dump_dir)\n",
    "\n",
    "        # iterate over all batch elements and create a CSV file for each one\n",
    "        max_t, max_b, max_c = rnn_output.shape\n",
    "        for b in range(max_b):\n",
    "            csv = ''\n",
    "            for t in range(max_t):\n",
    "                for c in range(max_c):\n",
    "                    csv += str(rnn_output[t, b, c]) + ';'\n",
    "                csv += '\\n'\n",
    "            fn = dump_dir + 'rnnOutput_' + str(b) + '.csv'\n",
    "            #print('Write dump of NN to file: ' + fn)\n",
    "            with open(fn, 'w') as f:\n",
    "                f.write(csv)\n",
    "\n",
    "    def infer_batch(self, batch: Batch, calc_probability: bool = False, probability_of_gt: bool = False):\n",
    "        #Feed a batch into the NN to recognize the texts\n",
    "\n",
    "        # decode, optionally save RNN output\n",
    "        num_batch_elements = len(batch.imgs)\n",
    "\n",
    "        # put tensors to be evaluated into list\n",
    "        eval_list = []\n",
    "        eval_list.append(self.decoder)\n",
    "\n",
    "        if self.dump or calc_probability:\n",
    "            eval_list.append(self.ctc_in_3d_tbc)\n",
    "\n",
    "        # sequence length depends on input image size (model downsizes width by 4)\n",
    "        max_text_len = batch.imgs[0].shape[0] // 4\n",
    "\n",
    "        # dict containing all tensor fed into the model\n",
    "        feed_dict = {self.input_imgs: batch.imgs, self.seq_len: [max_text_len] * num_batch_elements,\n",
    "                     self.is_train: False}\n",
    "\n",
    "        # evaluate model\n",
    "        eval_res = self.sess.run(eval_list, feed_dict)\n",
    "        decoded = eval_res[0]\n",
    "\n",
    "        # map labels (numbers) to character string\n",
    "        texts = self.decoder_output_to_text(decoded, num_batch_elements)\n",
    "\n",
    "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
    "        probs = None\n",
    "        if calc_probability:\n",
    "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
    "            ctc_input = eval_res[1]\n",
    "            eval_list = self.loss_per_element\n",
    "            feed_dict = {self.saved_ctc_input: ctc_input, self.gt_texts: sparse,\n",
    "                         self.seq_len: [max_text_len] * num_batch_elements, self.is_train: False}\n",
    "            loss_vals = self.sess.run(eval_list, feed_dict)\n",
    "            probs = np.exp(-loss_vals)\n",
    "\n",
    "        # dump the output of the NN to CSV file(s)\n",
    "        if self.dump:\n",
    "            self.dump_nn_output(eval_res[1])\n",
    "\n",
    "        return texts, probs\n",
    "\n",
    "    def save(self) -> None:\n",
    "        #Save model to file\n",
    "        self.snap_ID += 1\n",
    "        self.saver.save(self.sess, '/model/snapshot', global_step=self.snap_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePaths:\n",
    "    #Filenames and paths to data\n",
    "    fn_char_list = '/model/charList.txt'\n",
    "    fn_summary = '/model/summary.json'\n",
    "    fn_corpus = '/data/corpus.txt'\n",
    "    fn_train =\"/data/\"\n",
    "\n",
    "\n",
    "def get_img_height() -> int:\n",
    "    return 32\n",
    "\n",
    "\n",
    "def get_img_size() -> Tuple[int, int]:\n",
    "    #Height is fixed for NN, width is set according to training mode (single words or text lines)\n",
    "    if line_mode1:\n",
    "        return 256, get_img_height()\n",
    "    return 128, get_img_height()\n",
    "\n",
    "\n",
    "def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
    "    #Writes training summary file for NN\n",
    "    with open(FilePaths.fn_summary, 'w') as f:\n",
    "        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)\n",
    "\n",
    "\n",
    "def char_list_from_file() -> List[str]:\n",
    "    with open(FilePaths.fn_char_list) as f:\n",
    "        return list(f.read())\n",
    "\n",
    "\n",
    "def train(model: Model,\n",
    "          loader: DataLoaderIAM,\n",
    "          line_mode: bool,\n",
    "          early_stopping: int = 25) -> None:\n",
    "    #Trains NN\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    summary_char_error_rates = []\n",
    "    summary_word_accuracies = []\n",
    "    preprocessor = Preprocessor(get_img_size(), data_augmentation=True)\n",
    "    best_char_error_rate = float('inf')  # best validation character error rate\n",
    "    no_improvement_since = 0  # number of epochs no improvement of character error rate occurred\n",
    "    # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.train_set()\n",
    "        while loader.has_next():\n",
    "            iter_info = loader.get_iterator_info()\n",
    "            batch = loader.get_next()\n",
    "            batch = preprocessor.process_batch(batch)\n",
    "            loss = model.train_batch(batch)\n",
    "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
    "\n",
    "        # validate\n",
    "        char_error_rate, word_accuracy = validate(model, loader, line_mode)\n",
    "\n",
    "        # write summary\n",
    "        summary_char_error_rates.append(char_error_rate)\n",
    "        summary_word_accuracies.append(word_accuracy)\n",
    "        write_summary(summary_char_error_rates, summary_word_accuracies)\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if char_error_rate < best_char_error_rate:\n",
    "            print('Character error rate improved, save model')\n",
    "            best_char_error_rate = char_error_rate\n",
    "            no_improvement_since = 0\n",
    "            model.save()\n",
    "        else:\n",
    "            print(f'Character error rate not improved, best so far: {char_error_rate * 100.0}%')\n",
    "            no_improvement_since += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if no_improvement_since >= early_stopping:\n",
    "            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model: Model, loader: DataLoaderIAM, line_mode: bool) -> Tuple[float, float]:\n",
    "    #Validates NN\n",
    "    print('Validate NN')\n",
    "    loader.validation_set()\n",
    "    preprocessor = Preprocessor(get_img_size())\n",
    "    num_char_err = 0\n",
    "    num_char_total = 0\n",
    "    num_word_ok = 0\n",
    "    num_word_total = 0\n",
    "    while loader.has_next():\n",
    "        iter_info = loader.get_iterator_info()\n",
    "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
    "        batch = loader.get_next()\n",
    "        batch = preprocessor.process_batch(batch)\n",
    "        recognized, _ = model.infer_batch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
    "            num_word_total += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
    "            num_char_err += dist\n",
    "            num_char_total += len(batch.gt_texts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    char_error_rate = num_char_err / num_char_total\n",
    "    word_accuracy = num_word_ok / num_word_total\n",
    "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
    "    return char_error_rate, word_accuracy\n",
    "\n",
    "\n",
    "def infer(model: Model, fn_img,output_word_list) -> None:\n",
    "    #Recognizes text in image provided by file path\n",
    "    img = Image.open(fn_img)\n",
    "    fn_img = fn_img[:-4]\n",
    "    fn_img = fn_img + \".png\"\n",
    "    img.save(fn_img)\n",
    "    img = cv2.imread(fn_img,0)\n",
    "    lines = crop_img_into_lines(img)\n",
    "    \n",
    "    \n",
    "    for line in lines:\n",
    "        if line is  None: \n",
    "            print(\"no image\")\n",
    "            \n",
    "        else:\n",
    "            preprocessor = Preprocessor(get_img_size(), dynamic_width=True, padding=16)\n",
    "            line = preprocessor.process_img(line)\n",
    "            batch = Batch([line], None, 1)\n",
    "            recognized, probability = model.infer_batch(batch, True)\n",
    "            print(f'Recognized: \"{recognized[0]}\"')\n",
    "            the_output(output_word_list,recognized[0])\n",
    "            StrToTxtFile(output_word_list,save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "batchSize = 100\n",
    "train1 = False\n",
    "validate1=False\n",
    "line_mode1=True\n",
    "fast1=True\n",
    "infer1= True\n",
    "decoder = \"beamsearch\"\n",
    "dump1= True\n",
    "early_stopping1= 15\n",
    "output_word_list=[]\n",
    "testpath= \"/test/scan11.png\"\n",
    "save_path= \"/test/result\"\n",
    "\n",
    "def main():\n",
    "    #Main function\n",
    "    tf1.reset_default_graph()\n",
    "\n",
    "    decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
    "                       'beamsearch': DecoderType.BeamSearch,\n",
    "                       'wordbeamsearch': DecoderType.WordBeamSearch}\n",
    "    decoder_type = decoder_mapping[decoder]\n",
    "\n",
    "    # train the model\n",
    "    if  train1:\n",
    "        loader = DataLoaderIAM(FilePaths.fn_train, batchSize,fast=True)\n",
    "\n",
    "        # when in line mode, take care to have a whitespace in the char list\n",
    "        char_list = loader.char_list\n",
    "        if line_mode1 and ' ' not in char_list:\n",
    "            char_list = [' '] + char_list\n",
    "\n",
    "        # save characters and words\n",
    "        with open(FilePaths.fn_char_list, 'w') as f:\n",
    "            f.write(''.join(char_list))\n",
    "\n",
    "        with open(FilePaths.fn_corpus, 'w') as f:\n",
    "            f.write(' '.join(loader.train_words + loader.validation_words))\n",
    "\n",
    "        model = Model(char_list, decoder_type)\n",
    "        train(model, loader, line_mode = True, early_stopping=15)\n",
    "\n",
    "    # evaluate it on the validation set\n",
    "    elif validate1:\n",
    "        loader = loader = DataLoaderIAM(FilePaths.fn_train, batchSize,fast= True)\n",
    "        model = Model(char_list_from_file(), decoder_type, must_restore=True)\n",
    "        validate(model, loader,line_mode = True)\n",
    "\n",
    "    # infer text on test image\n",
    "    elif infer1:\n",
    "        model = Model(char_list_from_file(), decoder_type, must_restore=True, dump=dump1)\n",
    "        infer(model,testpath,output_word_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
